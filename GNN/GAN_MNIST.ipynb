{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOjtakbvaYiW0Jc+cqsAFjH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Environment Setup"],"metadata":{"id":"RGTtPaeh19xN"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"MYTKqsbf1uM-","executionInfo":{"status":"ok","timestamp":1702968707473,"user_tz":-480,"elapsed":5333,"user":{"displayName":"黃茂勛","userId":"15068294769745416772"}}},"outputs":[],"source":["import os\n","import sys\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from keras.datasets import mnist\n","from keras.layers import Dense, Flatten, Reshape, LeakyReLU\n","from keras.models import Sequential\n","from keras.optimizers import Adam"]},{"cell_type":"markdown","source":["# Load Data"],"metadata":{"id":"PkzhO6Op2BaG"}},{"cell_type":"code","source":["from keras.datasets import mnist\n","\n","(X_train, _), (_, _) = mnist.load_data()\n","\n","# Rescale -1 to 1 to fit tanh() activation function of Output Layer\n","X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n","\n","# Add a color channel dimension, from (60000, 28, 28) to (60000, 28, 28, 1)\n","X_train =  np.expand_dims(X_train, axis=3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-qu8dWC2ASc","executionInfo":{"status":"ok","timestamp":1702968723201,"user_tz":-480,"elapsed":472,"user":{"displayName":"黃茂勛","userId":"15068294769745416772"}},"outputId":"584bb36d-d21f-4e69-bbbe-8e7185f1b637"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"markdown","source":["# Setup hyperparameter"],"metadata":{"id":"FlfOuVri2Dbr"}},{"cell_type":"code","source":["# Set the shape of MNIST\n","img_rows = 28\n","img_cols = 28\n","channels = 1\n","\n","img_shape = (img_rows, img_cols, channels)\n","\n","# Set the length of noise for GANs generator Input Layer\n","z_dim = 100\n","\n","# Set the number of training epochs\n","epochs = 10000\n","\n","# Set the batch size\n","batch_size = 128\n","\n","# Set how many times of training should show acc / loss once\n","sample_interval = 100"],"metadata":{"id":"y0hbBS_n2Fq8","executionInfo":{"status":"ok","timestamp":1702968733392,"user_tz":-480,"elapsed":1,"user":{"displayName":"黃茂勛","userId":"15068294769745416772"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"SxfL95352LWr"}},{"cell_type":"markdown","source":["## Generator"],"metadata":{"id":"Lq-1lrIO2MsU"}},{"cell_type":"code","source":["def build_generator(img_shape, z_dim):\n","    model = Sequential()\n","    model.add(Dense(128, input_dim=z_dim))\n","    model.add(LeakyReLU(alpha=0.01))\n","    model.add(Dense(np.prod(img_shape), activation='tanh'))\n","    model.add(Reshape(img_shape))\n","    return model"],"metadata":{"id":"RmqD4VuN2Kxz","executionInfo":{"status":"ok","timestamp":1702969221976,"user_tz":-480,"elapsed":468,"user":{"displayName":"黃茂勛","userId":"15068294769745416772"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Discriminator"],"metadata":{"id":"M5xGXKu42O6t"}},{"cell_type":"code","source":["def build_discriminator(img_shape):\n","    model = Sequential()\n","    model.add(Flatten(input_shape=img_shape))\n","    model.add(Dense(128))\n","    model.add(LeakyReLU(alpha=0.01))\n","    model.add(Dense(1, activation='sigmoid'))\n","    return model"],"metadata":{"id":"wDeuKUL12Tpm","executionInfo":{"status":"ok","timestamp":1702969222338,"user_tz":-480,"elapsed":4,"user":{"displayName":"黃茂勛","userId":"15068294769745416772"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Combination Function of Generator and Discriminator"],"metadata":{"id":"r3M5n6ZV39U1"}},{"cell_type":"code","source":["def build_gan(generator, discriminator):\n","    model = Sequential()\n","    model.add(generator)\n","    model.add(discriminator)\n","    return model"],"metadata":{"id":"io7rblMY4BUj","executionInfo":{"status":"ok","timestamp":1702969222674,"user_tz":-480,"elapsed":4,"user":{"displayName":"黃茂勛","userId":"15068294769745416772"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Build the Whole GAN"],"metadata":{"id":"lxZHL_a_4ERW"}},{"cell_type":"code","source":["# Create and Compile Discriminator\n","discriminator = build_discriminator(img_shape)\n","discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n","\n","# Create and Compile Generator\n","generator = build_generator(img_shape, z_dim)\n","\n","# Frozen the Discriminator's weights when training Generator\n","discriminator.trainable = False\n","\n","# Create and Compile GAN\n","gan = build_gan(generator, discriminator)\n","gan.compile(loss='binary_crossentropy', optimizer=Adam())"],"metadata":{"id":"2O1INVfj4CH9","executionInfo":{"status":"ok","timestamp":1702969244964,"user_tz":-480,"elapsed":1292,"user":{"displayName":"黃茂勛","userId":"15068294769745416772"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"LD5mnjsn4H6j"}},{"cell_type":"markdown","source":["## Show Images Function"],"metadata":{"id":"_tetbJEjE_Hv"}},{"cell_type":"code","source":["def sample_images(generator, image_grid_rows=4, image_grid_columns=4):\n","    # Sample random noise\n","    noise = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, z_dim))\n","\n","    # Generate images from random noise\n","    gen_imgs = generator.predict(noise)\n","\n","    # Rescale image pixel values to [0, 1]\n","    gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","    # Set image grid\n","    fig, axs = plt.subplots(image_grid_rows,\n","                            image_grid_columns,\n","                            figsize=(4, 4),\n","                            sharey=True,\n","                            sharex=True)\n","    cnt = 0\n","    for i in range(image_grid_rows):\n","        for j in range(image_grid_columns):\n","            # Output a grid of images\n","            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n","            axs[i, j].axis('off')\n","            cnt += 1\n","    plt.show()"],"metadata":{"id":"Mgw6O-kk4HT7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define the Training Function"],"metadata":{"id":"9TE80gDaFEFm"}},{"cell_type":"code","source":["losses = []\n","accuracies = []\n","iteration_checkpoints = []\n","\n","def train(iterations, batch_size, sample_interval):\n","    # Labels for real images: all ones\n","    real = np.ones((batch_size, 1))\n","\n","    # Labels for fake images: all zeros\n","    fake = np.zeros((batch_size, 1))\n","\n","    for iteration in range(iterations):\n","\n","        # Turn off the stdout temporarily\n","        # To supress 4/4 [=====================] - 0s 3ms/step\n","        original_stdout = sys.stdout\n","        sys.stdout = open(os.devnull, 'w')\n","\n","        # ---- Train the Discriminator ----\n","\n","        # Get a random batch of real images\n","        idx = np.random.randint(0, X_train.shape[0], batch_size)\n","        imgs = X_train[idx]\n","\n","        # Sample noise and generate a batch of fake images\n","        noise = np.random.normal(0, 1, (batch_size, z_dim))\n","        gen_imgs = generator.predict(noise)\n","\n","        # Train Discriminator\n","        d_loss_real = discriminator.train_on_batch(imgs, real)\n","        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n","        d_loss, accuracy = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","        # ---- Train the Generator ----\n","\n","        # Generate a batch of fake images\n","        noise = np.random.normal(0, 1, (batch_size, z_dim))\n","\n","        # Set the Noise as True in purpose\n","        g_loss = gan.train_on_batch(noise, real)\n","\n","        # Turn stdout back on\n","        sys.stdout.close()\n","        sys.stdout = original_stdout\n","\n","        if (iteration + 1) % sample_interval == 0:\n","\n","            # Save losses and accuracies so they can be plotted after training\n","            losses.append((d_loss, g_loss))\n","            accuracies.append(100.0 * accuracy)\n","            iteration_checkpoints.append(iteration + 1)\n","\n","            # Output training progress\n","            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" %\n","                (iteration + 1, d_loss, 100.0 * accuracy, g_loss))\n","\n","            # Output a sample of generated image\n","            sample_images(generator)"],"metadata":{"id":"PdRZOeqyFPNq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Start Training"],"metadata":{"id":"mDqc9UnDFSDY"}},{"cell_type":"code","source":["train(iterations=epochs, batch_size=batch_size, sample_interval=sample_interval)"],"metadata":{"id":"kKFrmH3FFTQX"},"execution_count":null,"outputs":[]}]}